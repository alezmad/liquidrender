# ISS-122: LLM Model Architecture Shift

**Issue Type:** Evolution Risk
**Category:** Extensibility and Evolution
**Priority:** High
**Affects:** Discovery Engine (Section 12), Tiered Resolution (Section 13), LLM Integration

---

## Problem Statement

LiquidCode v2 is optimized for **text-to-text LLMs** with **token efficiency** as a core value proposition:

- 114x token reduction (4000 → 35 tokens)
- Latency improvement (8-12s → 70-100ms)
- Cost reduction (99% cheaper)

**These assumptions may become obsolete** as LLM architectures evolve:

### Future LLM Architectures

| Evolution | Impact on LiquidCode |
|-----------|---------------------|
| **Multimodal (Vision + Text)** | Can process visual mockups directly |
| **Structured Output Mode** | Native JSON generation, bypassing LiquidCode |
| **Agentic (Tool Use)** | Iterative refinement, not one-shot generation |
| **Reasoning Models** | Internal reasoning tokens (not counted in output) |
| **Edge/Local Models** | Different cost model (latency matters, tokens don't) |

### Existential Question

**If future LLMs output JSON cheaply, does LiquidCode still have value?**

**Answer:** Yes, but the value proposition shifts:

1. **Primary Value:** Platform-agnostic interface specification language
2. **Secondary Value:** Token efficiency (may erode)
3. **Tertiary Value:** Constraint-based generation (guides LLM decisions)

---

## Resolution

### Strategy: Model-Agnostic Architecture

Position LiquidCode as the **specification layer**, not the compression layer.

### Implementation

#### 1. LLM Provider Abstraction

```typescript
interface LLMProvider {
  // Metadata
  name: string;
  version: string;
  capabilities: LLMCapabilities;

  // Text generation (current)
  generate(prompt: string, options?: GenerateOptions): Promise<string>;

  // Tokenization (for budgeting)
  tokenize(text: string): number;
  estimateCost(tokens: number): number;

  // Future: Structured output
  generateStructured?<T>(
    prompt: string,
    schema: z.ZodType<T>,
    options?: GenerateOptions
  ): Promise<T>;

  // Future: Multimodal
  generateMultimodal?(
    inputs: MultimodalInput[],
    options?: GenerateOptions
  ): Promise<string | StructuredOutput>;

  // Future: Agentic
  generateWithTools?(
    prompt: string,
    tools: Tool[],
    options?: GenerateOptions
  ): Promise<AgenticResult>;
}

interface LLMCapabilities {
  // Input modalities
  supportsText: boolean;
  supportsImages: boolean;
  supportsAudio: boolean;
  supportsVideo: boolean;

  // Output modes
  supportsStructuredOutput: boolean;
  supportsToolCalling: boolean;
  supportsStreaming: boolean;

  // Constraints
  maxContextLength: number;
  maxOutputTokens: number;
  costPer1kTokens: number;
}

interface MultimodalInput {
  type: 'text' | 'image' | 'data' | 'schema';
  content: any;
  metadata?: Record<string, any>;
}
```

#### 2. Structured Output Mode (Future)

When LLM supports native structured output:

```typescript
// Current (v2.0): Text → LiquidCode → LiquidSchema
const liquidCode = await llm.generate(prompt);
const schema = compiler.compile(liquidCode);

// Future (v3.0+): Direct LiquidSchema generation
if (llm.capabilities.supportsStructuredOutput) {
  const schema = await llm.generateStructured<LiquidSchema>(
    prompt,
    LiquidSchemaZod,  // Zod schema as constraint
    { maxTokens: 1000 }
  );

  // No compilation needed - LLM outputs valid schema directly
  // But: Still using LiquidSchema as the target format!
}
```

**Key insight:** LiquidSchema becomes the **constraint** for structured output, not the compilation target.

#### 3. Multimodal Input (Future)

When LLM supports vision:

```typescript
// User provides visual mockup + data
const inputs: MultimodalInput[] = [
  {
    type: 'text',
    content: 'Create a dashboard like this mockup'
  },
  {
    type: 'image',
    content: mockupImage,  // Screenshot or sketch
    metadata: { format: 'png' }
  },
  {
    type: 'data',
    content: dataFingerprint,  // Schema of available data
    metadata: { source: 'postgresql' }
  },
  {
    type: 'schema',
    content: LiquidSchemaZod,  // Target schema format
    metadata: { version: '3.0.0' }
  }
];

const result = await llm.generateMultimodal(inputs);

// LLM extracts layout from image, maps to data, outputs LiquidSchema
const schema: LiquidSchema = result.schema;
```

**Key insight:** LiquidSchema is the **output format** even when input is multimodal.

#### 4. Agentic Generation (Future)

When LLM uses tools to iterate:

```typescript
const tools: Tool[] = [
  {
    name: 'queryData',
    description: 'Query available data fields',
    execute: (query) => dataSource.query(query)
  },
  {
    name: 'previewInterface',
    description: 'Render preview of LiquidSchema',
    execute: (schema) => adapter.render(schema, sampleData)
  },
  {
    name: 'validateSchema',
    description: 'Validate LiquidSchema correctness',
    execute: (schema) => validator.validate(schema)
  }
];

const result = await llm.generateWithTools(
  'Create a sales dashboard',
  tools
);

// LLM iteratively:
// 1. Queries data to understand schema
// 2. Generates initial LiquidSchema
// 3. Previews render
// 4. Refines schema
// 5. Validates
// 6. Returns final schema

const schema: LiquidSchema = result.output;
```

**Key insight:** LiquidSchema is the **target format** even when generation is iterative.

#### 5. Adaptive Token Budgeting

```typescript
interface TokenBudget {
  mode: 'strict' | 'adaptive' | 'unlimited';
  maxTokens: number;
  costLimit?: number;  // $/request
}

class AdaptiveTokenBudgeter {
  calculateBudget(llm: LLMProvider, intent: Intent): TokenBudget {
    // If structured output available, token budget is less critical
    if (llm.capabilities.supportsStructuredOutput) {
      return {
        mode: 'adaptive',
        maxTokens: 1000,  // Generous
        costLimit: 0.01
      };
    }

    // If text-to-text, token efficiency matters
    return {
      mode: 'strict',
      maxTokens: 100,  // Tight
      costLimit: 0.001
    };
  }
}
```

#### 6. Value Proposition Evolution

```typescript
// V2.0 (2025): Token efficiency is primary value
const valueProposition = {
  primary: 'Token efficiency (114x reduction)',
  secondary: 'Platform-agnostic schema',
  tertiary: 'Constraint-based generation'
};

// V3.0 (2027): Schema format is primary value
const valueProposition = {
  primary: 'Universal interface specification language',
  secondary: 'Platform-agnostic (works with any LLM)',
  tertiary: 'Token efficiency (where applicable)'
};

// V4.0 (2029): Ecosystem is primary value
const valueProposition = {
  primary: 'Ecosystem of blocks, signals, adapters',
  secondary: 'Works with any LLM architecture',
  tertiary: 'Constraint-based generation quality'
};
```

---

## LiquidCode's Enduring Value (Beyond Tokens)

Even if token efficiency becomes irrelevant:

### 1. Platform Abstraction
LiquidSchema decouples interface specification from rendering platform (React, Native, Qt, etc.).

### 2. Constraint Specification
LiquidCode provides **semantic constraints** (binding slots, signal types, layout priorities) that guide LLM decisions, improving quality regardless of output format.

### 3. Validation & Type Safety
Zod schemas ensure generated interfaces are valid, preventing runtime errors.

### 4. Composition & Caching
Fragment composition and caching work regardless of how fragments are generated.

### 5. Mutation Algebra
Interface algebra (add, remove, modify) is **independent** of generation method.

### 6. Adapter Ecosystem
Once adapters exist for multiple platforms, switching generation method doesn't break them.

---

## Migration Path

### Phase 1: Provider Abstraction (v2.1)
- Implement `LLMProvider` interface
- Abstract current text-to-text generation
- Add capability detection
- No functional change

### Phase 2: Structured Output Support (v2.5)
- Add `generateStructured()` method
- Detect structured output capability
- Use when available, fall back to text
- Token budgets become adaptive

### Phase 3: Multimodal Support (v3.0)
- Add `generateMultimodal()` method
- Support image inputs (mockups, sketches)
- LiquidSchema remains output format

### Phase 4: Agentic Support (v3.5)
- Add `generateWithTools()` method
- Provide tools for data query, preview, validation
- Iterative refinement of LiquidSchema

---

## Testing Requirements

```typescript
describe('LLM Provider Abstraction', () => {
  it('uses structured output when available', async () => {
    const llm: LLMProvider = {
      capabilities: { supportsStructuredOutput: true },
      generateStructured: jest.fn().mockResolvedValue(mockSchema)
    };

    const schema = await engine.generate(intent, llm);

    expect(llm.generateStructured).toHaveBeenCalled();
    expect(schema).toEqual(mockSchema);
  });

  it('falls back to text generation', async () => {
    const llm: LLMProvider = {
      capabilities: { supportsStructuredOutput: false },
      generate: jest.fn().mockResolvedValue(liquidCode)
    };

    const schema = await engine.generate(intent, llm);

    expect(llm.generate).toHaveBeenCalled();
    expect(compiler.compile).toHaveBeenCalled();
  });

  it('supports multimodal input', async () => {
    const llm: LLMProvider = {
      capabilities: { supportsImages: true },
      generateMultimodal: jest.fn().mockResolvedValue({ schema: mockSchema })
    };

    const schema = await engine.generateFromMockup(mockupImage, data, llm);

    expect(llm.generateMultimodal).toHaveBeenCalledWith(
      expect.arrayContaining([
        expect.objectContaining({ type: 'image' })
      ])
    );
  });
});
```

---

## Documentation Requirements

### LLM Provider Guide

**Title:** "Integrating LLM Providers with LiquidCode"

**Contents:**
1. Provider interface specification
2. Text-to-text providers (current)
3. Structured output providers (future)
4. Multimodal providers (future)
5. Agentic providers (future)
6. Capability detection
7. Fallback strategies

### Value Proposition Evolution

**Title:** "LiquidCode in the Age of Advanced LLMs"

**Contents:**
1. Why LiquidCode still matters (beyond tokens)
2. Platform abstraction value
3. Constraint-based quality
4. Ecosystem benefits
5. Future-proofing strategy

---

## Implementation Checklist

### Phase 1 (v2.1)
- [ ] Define `LLMProvider` interface
- [ ] Define `LLMCapabilities`
- [ ] Abstract current text generation
- [ ] Add capability detection
- [ ] Document provider interface

### Phase 2 (v2.5)
- [ ] Add `generateStructured()` method
- [ ] Implement adaptive token budgeting
- [ ] Test with structured output LLMs
- [ ] Update documentation

### Future Phases
- [ ] Multimodal support (v3.0)
- [ ] Agentic support (v3.5)
- [ ] Continuous provider evolution

---

## Resolution Status

**Status:** Resolved
**Specification Impact:** High (changes LLM integration architecture)
**Breaking Changes:** None (abstraction is backward compatible)
**Version Target:** v2.1 (abstraction), v2.5+ (new capabilities)

**Rationale Integration:** LiquidCode's value must **transcend token efficiency**. As LLM architectures evolve, LiquidCode remains relevant as:
1. A **specification language** (platform-agnostic interface definition)
2. A **constraint system** (guides LLM quality regardless of output mode)
3. An **ecosystem** (blocks, signals, adapters that work across LLM generations)

**Philosophy:** **LiquidCode is not married to any LLM architecture.** It's a specification language that happens to be optimized for text-to-text LLMs today, but will adapt to multimodal, structured output, and agentic LLMs tomorrow. The core primitives (Block, Slot, Signal) and the schema format (LiquidSchema) are architecture-agnostic.

**Strategic Positioning:**
- **2025:** "114x more efficient than raw JSON"
- **2027:** "Works with any LLM (text, structured, multimodal)"
- **2029:** "The standard for LLM-generated interfaces"
