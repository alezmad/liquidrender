# ISS-051: Latency Claims - LLM Generation Time

## Issue Summary

**Type:** Accuracy / Realism
**Severity:** High
**Location:** Executive Summary (§1.1) and Tiered Resolution (§13)
**Status:** RESOLVED

## Problem Statement

The specification claims aggressive latency targets:

**§1.1 Performance Metrics:**
```
| Latency | Traditional | LiquidCode | Improvement |
|---------|-------------|------------|-------------|
| 8-12s   | 70-100ms    | 100x faster |
```

**§13.1 Tiered Resolution Latencies:**
```
TIER 1: Cache Hit         <5ms
TIER 2: Semantic Search   <50ms
TIER 3: Composition       <100ms
TIER 4: LLM Generation    <500ms
```

**The Problem:**

1. **Tier 4 (LLM) at 500ms is unrealistic** for most LLM APIs in 2025:
   - GPT-4 Turbo: ~2-4s for 35 tokens (with network latency)
   - Claude Opus: ~1-3s for 35 tokens
   - Even fast models (GPT-3.5): ~500-800ms under ideal conditions

2. **The claimed 70-100ms overall latency** assumes >99% cache hit rate, which is:
   - Not representative of cold start or new data sources
   - Overly optimistic for production workloads
   - Misleading marketing if presented as typical

3. **No caveats or confidence intervals** - presented as guaranteed performance

## Analysis

### Reality Check: LLM Latency Components

For a 35-token LiquidCode generation:

| Component | Best Case | Typical | Worst Case |
|-----------|-----------|---------|------------|
| Network RTT | 20ms | 50ms | 200ms |
| API queue time | 0ms | 100ms | 2000ms |
| Model inference | 100ms | 300ms | 1000ms |
| Token generation (35 tokens) | 50ms | 150ms | 500ms |
| **TOTAL (Tier 4)** | **170ms** | **600ms** | **3700ms** |

**Conclusion:** The 500ms target for Tier 4 is achievable only in the **best case** (warm model, low queue, edge deployment). **Typical is 600-1000ms.**

### What About Tiered Resolution?

The spec's defense is that **most queries hit cache** (Tier 1-3):

```
TIER 1: 40% (cache hit)         <5ms
TIER 2: 50% (semantic match)    <50ms
TIER 3: 9% (composition)        <100ms
TIER 4: 1% (LLM fallback)       <500ms (UNREALISTIC)
```

**Analysis of Distribution:**

- **Cold start:** 0% Tier 1-3 hits → 100% Tier 4 → ~600-1000ms latency
- **After discovery warmup:** Distribution plausible, but:
  - Tier 2 (semantic search) may take 100-200ms with large vector DBs
  - Tier 3 (composition) includes micro-LLM calls (§13.4) which add latency
  - Tier 4 still needs realistic targets

### Root Cause

The latency claims suffer from:

1. **Best-case bias** - Using edge cases as headline numbers
2. **Missing confidence intervals** - No P50/P90/P99 breakdown
3. **Unrealistic Tier 4** - Doesn't account for API variability
4. **Overstated cache hit rates** - 99% is very aggressive

## Resolution

### Decision

**ADD REALISTIC LATENCY BANDS WITH CAVEATS** - Replace point estimates with ranges and percentiles.

### Updated §1.1 Performance Table

Replace the latency row with:

```markdown
| Metric | Traditional | LiquidCode (Cold)¹ | LiquidCode (Warm)² | Improvement |
|--------|-------------|--------------------|--------------------|-------------|
| Latency (P50) | 8-12s | 600-1000ms | 50-150ms | **10-100x faster** |
| Latency (P90) | 12-20s | 1000-2000ms | 100-300ms | **10-60x faster** |
| Latency (P99) | 20-40s | 2000-4000ms | 300-600ms | **7-60x faster** |

¹ Cold start: No cache, direct to LLM (Tier 4)
² Warm state: After discovery engine has pre-generated fragments (Tier 1-3 hits)
```

### Updated §13.1 Tiered Resolution Latencies

Replace the diagram with:

```markdown
### 13.1 Resolution Tiers and Realistic Latencies

```
User Intent
    ↓
┌───────────────────────────────────────────────────────────────────┐
│ TIER 1: Exact Cache Hit (Target: 40-60% of requests)              │
│   Intent hash matches cached fragment                             │
│   Latency: P50=3ms, P90=10ms, P99=50ms                           │
└───────────────────────────────────────────────────────────────────┘
    ↓ (miss)
┌───────────────────────────────────────────────────────────────────┐
│ TIER 2: Semantic Search (Target: 30-50% of requests)              │
│   Similar intent in vector store                                  │
│   Latency: P50=50ms, P90=150ms, P99=500ms                        │
│   (Includes embedding generation + vector search + coherence)     │
└───────────────────────────────────────────────────────────────────┘
    ↓ (miss)
┌───────────────────────────────────────────────────────────────────┐
│ TIER 3: Fragment Composition (Target: 5-15% of requests)          │
│   Combine cached fragments + micro-LLM for bindings               │
│   Latency: P50=200ms, P90=500ms, P99=1000ms                      │
│   (Includes composition logic + micro-LLM calls ~100ms each)      │
└───────────────────────────────────────────────────────────────────┘
    ↓ (miss)
┌───────────────────────────────────────────────────────────────────┐
│ TIER 4: LLM Generation (Target: 1-10% of requests)                │
│   Novel archetypes, cold start, or complex requirements           │
│   Latency: P50=600ms, P90=1200ms, P99=3000ms                     │
│   (Highly variable: depends on API load, model, region)           │
└───────────────────────────────────────────────────────────────────┘
```

**Note on Tier 4 Latency:**

LLM generation latency depends on:
- **Model choice** - Fast models (GPT-3.5-turbo) vs. capable models (GPT-4, Claude Opus)
- **API load** - Queue times vary from 0ms (dedicated) to 2000ms+ (shared)
- **Region** - Edge deployment (50ms RTT) vs. cross-continent (200ms RTT)
- **Token count** - 35 tokens (typical) vs. 100+ tokens (complex)

**Optimization strategies:**
- Use streaming to show progressive results (perceived latency <100ms)
- Deploy edge inference for <200ms Tier 4 (at higher cost)
- Prioritize cache warming to reduce Tier 4 traffic
```

### Add Latency Expectations Section

Add new section §13.5:

```markdown
## 13.5 Latency Expectations by Scenario

### Cold Start (First Query, No Cache)

| Scenario | Expected Latency | Tier |
|----------|------------------|------|
| Simple dashboard | 600-1000ms | Tier 4 (LLM) |
| Complex multi-view | 1000-2000ms | Tier 4 (LLM, parallel) |
| With discovery pre-warming | 50-200ms | Tier 2-3 (cache/composition) |

### Warm State (After Discovery)

| Scenario | Expected Latency | Tier |
|----------|------------------|------|
| Common pattern (cached) | 5-50ms | Tier 1 (cache hit) |
| Similar to cached (semantic) | 50-200ms | Tier 2 (semantic + coherence) |
| Novel variant | 200-600ms | Tier 3 (composition + micro-LLM) |
| Completely new pattern | 600-2000ms | Tier 4 (full LLM) |

### Mutations

| Scenario | Expected Latency | Tier |
|----------|------------------|------|
| Label change (L2) | <10ms | Deterministic (no LLM) |
| Block type swap | 50-100ms | Tier 3 (micro-LLM) |
| Add block with binding | 100-300ms | Tier 3-4 (micro-LLM or composition) |
| Full restructure (L0) | 600-1000ms | Tier 4 (LLM) |

### Optimized Deployment (Edge Inference)

With edge-deployed models (higher cost):
- Tier 4 latency: P50=200ms, P90=400ms, P99=800ms
- Overall P50: 20-80ms (assuming 90% Tier 1-3 hit rate)
```

### Update §1.1 Cost Metric

Also clarify the cost metric to reflect Tier 4:

```markdown
| Metric | Traditional | LiquidCode (Warm) | Improvement |
|--------|-------------|-------------------|-------------|
| Cost per generation | $0.12 | $0.001-0.005 | **99% cheaper** |

Cost breakdown:
- Tier 1-3 (cache/semantic/composition): ~$0.001 (vector search + micro-LLM)
- Tier 4 (full LLM): ~$0.005 (GPT-4 Turbo for 35 tokens)
- Weighted average (90% Tier 1-3, 10% Tier 4): ~$0.0015

Traditional cost assumes full JSON generation (~4000 tokens × $0.03/1K = $0.12).
```

## Verification

### Benchmark Requirements

Before production, measure these scenarios and publish results:

| Scenario | Measure | Acceptance Criteria |
|----------|---------|---------------------|
| Cache hit (Tier 1) | P50/P90/P99 latency | P90 < 10ms |
| Semantic match (Tier 2) | P50/P90/P99 latency | P90 < 200ms |
| Composition (Tier 3) | P50/P90/P99 latency | P90 < 500ms |
| LLM generation (Tier 4) | P50/P90/P99 latency | P90 < 1500ms |
| Cold start | End-to-end latency | P90 < 2000ms |
| Warm state (90% cache) | End-to-end latency | P90 < 200ms |

### Test Configuration

Benchmarks MUST specify:
- LLM model and API (e.g., "GPT-4 Turbo via OpenAI API, US-East region")
- Network conditions (e.g., "50ms RTT, 100Mbps")
- Cache hit rate achieved (e.g., "85% Tier 1-3 after 24h warmup")
- Load conditions (e.g., "Single user, no API queue")

## Impact Assessment

### Sections Modified

- **§1.1** - Performance table updated with ranges and caveats
- **§13.1** - Tiered resolution latencies updated with P50/P90/P99
- **§13.5** (NEW) - Latency expectations by scenario

### Breaking Changes

None - this is a **documentation correction**, not an API change.

### Marketing Impact

**Before (Misleading):**
> "LiquidCode is 100x faster than traditional methods, generating interfaces in 70-100ms."

**After (Honest):**
> "LiquidCode reduces latency from 8-12s to 50-1000ms depending on cache state, achieving 10-100x speedup. Cold start is ~600ms (LLM tier), warm state with cache hits is ~50ms (cache tier)."

**Recommendation:** Use "10-100x faster" in marketing, with footnote explaining cache dependency.

## Sign-off

- [x] Latency claims updated with realistic ranges
- [x] Percentiles (P50/P90/P99) added for transparency
- [x] Caveats for LLM API variability included
- [x] Tier 4 targets raised from 500ms → 600-1200ms (P50-P90)
- [x] Scenario-based expectations documented
- [x] Benchmark requirements specified

---

**Resolved By:** LiquidCode Analysis Team
**Date:** 2025-12-21
**Resolution Type:** Specification Correction (Accuracy Improvement)
